{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9155da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bfe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = \"idopinto/rlinv_train_sft_test\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "print(f\"Loaded dataset {dataset_name}...\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"openai/gpt-oss-20b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Loaded tokenizer {tokenizer}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e541f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Mxfp4Config\n",
    "quantization_config = Mxfp4Config(dequantize=False)\n",
    "print(f\"Loaded quantization config {quantization_config}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    # dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "print(f\"Loaded model {model}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f209f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "model_name = \"openai/gpt-oss-20b\"\n",
    "trained_model_name = \"gpt-oss-20b-rlinv-sft\"\n",
    "dataset_name = \"idopinto/rlinv_train_sft_test\"\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    target_parameters=[\n",
    "        \"7.mlp.experts.gate_up_proj\",\n",
    "        \"7.mlp.experts.down_proj\",\n",
    "        \"15.mlp.experts.gate_up_proj\",\n",
    "        \"15.mlp.experts.down_proj\",\n",
    "        \"23.mlp.experts.gate_up_proj\",\n",
    "        \"23.mlp.experts.down_proj\",\n",
    "    ],\n",
    ")\n",
    "training_args = SFTConfig(\n",
    "    learning_rate=2e-4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "    output_dir=trained_model_name,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ecc7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
