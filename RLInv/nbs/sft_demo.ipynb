{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff11667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-butterfly-2</strong> at: <a href='https://wandb.ai/ido-pinto/rlinv-sft-demo/runs/pkbxkwza' target=\"_blank\">https://wandb.ai/ido-pinto/rlinv-sft-demo/runs/pkbxkwza</a><br> View project at: <a href='https://wandb.ai/ido-pinto/rlinv-sft-demo' target=\"_blank\">https://wandb.ai/ido-pinto/rlinv-sft-demo</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251109_171251-pkbxkwza/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/labs/guykatz/idopinto12/projects/loop_invariant_generation/RLInv/wandb/run-20251109_172501-6am3ayb4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ido-pinto/rlinv-sft-demo/runs/6am3ayb4' target=\"_blank\">colorful-water-3</a></strong> to <a href='https://wandb.ai/ido-pinto/rlinv-sft-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ido-pinto/rlinv-sft-demo' target=\"_blank\">https://wandb.ai/ido-pinto/rlinv-sft-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ido-pinto/rlinv-sft-demo/runs/6am3ayb4' target=\"_blank\">https://wandb.ai/ido-pinto/rlinv-sft-demo/runs/6am3ayb4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ido-pinto/rlinv-sft-demo/runs/6am3ayb4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f9f791fdbd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.1 Setup WANDB for training metrics\n",
    "os.environ[\"WANDB_PROJECT\"] = \"rlinv-sft-demo\"  # Change to your desired project!\n",
    "wandb.init(project=os.environ[\"WANDB_PROJECT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04db8ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c11dfd734da498b8a030b0ae99e59c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login \n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f51fafbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 1000\n",
      "Sample: {'reasoning_language': 'French', 'developer': 'You are an AI chatbot with a lively and energetic personality.', 'user': 'Can you show me the latest trends on Twitter right now?', 'analysis': \"D'accord, l'utilisateur demande les tendances Twitter les plus r√©centes. Tout d'abord, je dois v√©rifier si j'ai acc√®s √† des donn√©es en temps r√©el. √âtant donn√© que je ne peux pas naviguer sur Internet ou acc√©der directement √† l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils g√©n√©raux sur la fa√ßon de les trouver.\\n\\nJe devrais pr√©ciser que les tendances Twitter √©voluent rapidement et sont sp√©cifiques √† chaque r√©gion. Je pourrais sugg√©rer de consulter la section ¬´\\xa0En vogue\\xa0¬ª sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient √™tre utiles. Il est important de souligner que les tendances varient selon la r√©gion et l'heure de la journ√©e. Je devrais garder un ton amical et bienveillant, peut-√™tre ajouter un emoji pour rester l√©ger. Je vais structurer ma r√©ponse √©tape par √©tape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des donn√©es en temps r√©el et proposer d'autres m√©thodes. Je conserverai un langage simple et convivial, en √©vitant les termes techniques.\", 'final': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website ‚Äì it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what‚Äôs blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** ‚Äì they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I‚Äôve got ideas!', 'messages': [{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.', 'role': 'system', 'thinking': None}, {'content': 'Can you show me the latest trends on Twitter right now?', 'role': 'user', 'thinking': None}, {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website ‚Äì it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what‚Äôs blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** ‚Äì they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I‚Äôve got ideas!', 'role': 'assistant', 'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus r√©centes. Tout d'abord, je dois v√©rifier si j'ai acc√®s √† des donn√©es en temps r√©el. √âtant donn√© que je ne peux pas naviguer sur Internet ou acc√©der directement √† l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils g√©n√©raux sur la fa√ßon de les trouver.\\n\\nJe devrais pr√©ciser que les tendances Twitter √©voluent rapidement et sont sp√©cifiques √† chaque r√©gion. Je pourrais sugg√©rer de consulter la section ¬´\\xa0En vogue\\xa0¬ª sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient √™tre utiles. Il est important de souligner que les tendances varient selon la r√©gion et l'heure de la journ√©e. Je devrais garder un ton amical et bienveillant, peut-√™tre ajouter un emoji pour rester l√©ger. Je vais structurer ma r√©ponse √©tape par √©tape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des donn√©es en temps r√©el et proposer d'autres m√©thodes. Je conserverai un langage simple et convivial, en √©vitant les termes techniques.\"}]}\n"
     ]
    }
   ],
   "source": [
    "# 1. LOAD DATASET\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "print(f\"Total examples: {len(dataset)}\")\n",
    "print(\"Sample:\", dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddc5c60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.',\n",
       "  'role': 'system',\n",
       "  'thinking': None},\n",
       " {'content': 'Can you show me the latest trends on Twitter right now?',\n",
       "  'role': 'user',\n",
       "  'thinking': None},\n",
       " {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website ‚Äì it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what‚Äôs blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** ‚Äì they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I‚Äôve got ideas!',\n",
       "  'role': 'assistant',\n",
       "  'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus r√©centes. Tout d'abord, je dois v√©rifier si j'ai acc√®s √† des donn√©es en temps r√©el. √âtant donn√© que je ne peux pas naviguer sur Internet ou acc√©der directement √† l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils g√©n√©raux sur la fa√ßon de les trouver.\\n\\nJe devrais pr√©ciser que les tendances Twitter √©voluent rapidement et sont sp√©cifiques √† chaque r√©gion. Je pourrais sugg√©rer de consulter la section ¬´\\xa0En vogue\\xa0¬ª sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient √™tre utiles. Il est important de souligner que les tendances varient selon la r√©gion et l'heure de la journ√©e. Je devrais garder un ton amical et bienveillant, peut-√™tre ajouter un emoji pour rester l√©ger. Je vais structurer ma r√©ponse √©tape par √©tape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des donn√©es en temps r√©el et proposer d'autres m√©thodes. Je conserverai un langage simple et convivial, en √©vitant les termes techniques.\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = dataset[0]['messages']\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35460919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-09\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "reasoning language: French\n",
      "\n",
      "You are an AI chatbot with a lively and energetic personality.\n",
      "\n",
      "<|end|><|start|>user<|message|>Can you show me the latest trends on Twitter right now?<|end|><|start|>assistant<|channel|>analysis<|message|>D'accord, l'utilisateur demande les tendances Twitter les plus r√©centes. Tout d'abord, je dois v√©rifier si j'ai acc√®s √† des donn√©es en temps r√©el. √âtant donn√© que je ne peux pas naviguer sur Internet ou acc√©der directement √† l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils g√©n√©raux sur la fa√ßon de les trouver.\n",
      "\n",
      "Je devrais pr√©ciser que les tendances Twitter √©voluent rapidement et sont sp√©cifiques √† chaque r√©gion. Je pourrais sugg√©rer de consulter la section ¬´¬†En vogue¬†¬ª sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient √™tre utiles. Il est important de souligner que les tendances varient selon la r√©gion et l'heure de la journ√©e. Je devrais garder un ton amical et bienveillant, peut-√™tre ajouter un emoji pour rester l√©ger. Je vais structurer ma r√©ponse √©tape par √©tape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des donn√©es en temps r√©el et proposer d'autres m√©thodes. Je conserverai un langage simple et convivial, en √©vitant les termes techniques.<|end|><|start|>assistant<|channel|>final<|message|>Hey there!  While I can't check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\n",
      "\n",
      "1. **Open the \"Trending\" tab** on the Twitter app or website ‚Äì it updates constantly!  \n",
      "2. **Search for hashtags** like #Trending or #Viral to see what‚Äôs blowing up.  \n",
      "3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \n",
      "4. **Check regional trends** ‚Äì they often differ by location!  \n",
      "\n",
      "Remember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \n",
      "\n",
      "Want me to brainstorm *what* might trend next? I‚Äôve got ideas!<|return|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
    "conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(conversation)  # Shows how dialogue is formatted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8ffa3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Deleted model\n",
      "‚úì Deleted peft_model\n",
      "‚úì Deleted trainer\n",
      "‚úì Deleted tokenizer\n",
      "‚úì Deleted dataset\n",
      "‚úì Python garbage collection completed\n",
      "‚úì CUDA cache cleared and stats reset\n",
      "\n",
      "üìä GPU Memory Status:\n",
      "   Allocated: 1.91 GB\n",
      "   Reserved: 45.79 GB\n",
      "   Free: 94.01 GB\n",
      "\n",
      "‚úÖ Memory cleanup complete! You can now start fresh.\n"
     ]
    }
   ],
   "source": [
    "# MEMORY CLEANUP - Run this cell to free GPU memory and start clean\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"Comprehensive memory cleanup function\"\"\"\n",
    "    # Delete common model variables if they exist\n",
    "    variables_to_delete = ['model', 'peft_model', 'trainer', 'tokenizer', 'dataset']\n",
    "    for var_name in variables_to_delete:\n",
    "        if var_name in globals():\n",
    "            del globals()[var_name]\n",
    "            print(f\"‚úì Deleted {var_name}\")\n",
    "    \n",
    "    # Python garbage collection\n",
    "    gc.collect()\n",
    "    print(\"‚úì Python garbage collection completed\")\n",
    "    \n",
    "    # CUDA cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"‚úì CUDA cache cleared and stats reset\")\n",
    "        \n",
    "        # Print memory status\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"\\nüìä GPU Memory Status:\")\n",
    "        print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"   Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"   Free: {torch.cuda.get_device_properties(0).total_memory / 1024**3 - reserved:.2f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö† CUDA not available\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Memory cleanup complete! You can now start fresh.\")\n",
    "\n",
    "# Run cleanup\n",
    "clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8987acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Deleted torch\n",
      "‚úì Deleted param\n",
      "‚úì Deleted test_input\n",
      "‚úì Deleted test_loss\n",
      "‚úì GC round 1: collected 482 objects\n",
      "\n",
      "üìä GPU Memory after aggressive cleanup:\n",
      "   Allocated: 0.83 GB\n",
      "   Reserved: 45.79 GB\n",
      "\n",
      "‚úÖ Aggressive cleanup complete! Deleted 4 variables.\n",
      "üí° If memory is still high, consider restarting the kernel (Kernel ‚Üí Restart)\n"
     ]
    }
   ],
   "source": [
    "# AGGRESSIVE MEMORY CLEANUP - Use if regular cleanup doesn't work\n",
    "# This will try to delete all PyTorch/transformers objects\n",
    "\n",
    "def aggressive_cleanup():\n",
    "    \"\"\"More aggressive cleanup that searches for large objects\"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    import sys\n",
    "    \n",
    "    # Get all variables in current namespace\n",
    "    current_vars = list(globals().keys())\n",
    "    \n",
    "    # Delete any object that might be holding GPU memory\n",
    "    deleted_count = 0\n",
    "    for var_name in current_vars:\n",
    "        if var_name.startswith('_'):\n",
    "            continue\n",
    "        try:\n",
    "            obj = globals()[var_name]\n",
    "            # Check if it's a model, tensor, or large object\n",
    "            if hasattr(obj, 'cuda') or hasattr(obj, 'to'):\n",
    "                del globals()[var_name]\n",
    "                deleted_count += 1\n",
    "                print(f\"‚úì Deleted {var_name}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Multiple rounds of garbage collection\n",
    "    for i in range(3):\n",
    "        collected = gc.collect()\n",
    "        if collected > 0:\n",
    "            print(f\"‚úì GC round {i+1}: collected {collected} objects\")\n",
    "    \n",
    "    # CUDA cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"\\nüìä GPU Memory after aggressive cleanup:\")\n",
    "        print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"   Reserved: {reserved:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Aggressive cleanup complete! Deleted {deleted_count} variables.\")\n",
    "    print(\"üí° If memory is still high, consider restarting the kernel (Kernel ‚Üí Restart)\")\n",
    "\n",
    "# Uncomment to run aggressive cleanup:\n",
    "aggressive_cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5700aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMORY STATUS CHECKER - Interpret your current GPU memory state\n",
    "import torch\n",
    "\n",
    "def check_memory_status():\n",
    "    \"\"\"Check and interpret current GPU memory status\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"‚ö† CUDA not available\")\n",
    "        return\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    free = total - reserved\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä CURRENT GPU MEMORY STATUS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total GPU Memory:     {total:.2f} GB\")\n",
    "    print(f\"Reserved Memory:      {reserved:.2f} GB\")\n",
    "    print(f\"Allocated Memory:     {allocated:.2f} GB\")\n",
    "    print(f\"Available Memory:     {free:.2f} GB\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\nüí° INTERPRETATION:\")\n",
    "    \n",
    "    if allocated < 1:\n",
    "        print(\"‚úÖ Excellent! Almost no memory allocated.\")\n",
    "    elif allocated < 10:\n",
    "        print(\"‚úÖ Good! Low memory usage.\")\n",
    "    elif allocated < 50:\n",
    "        print(\"‚ö†Ô∏è  Moderate memory usage.\")\n",
    "    else:\n",
    "        print(\"‚ùå High memory usage - consider cleanup!\")\n",
    "    \n",
    "    if reserved > total * 0.8:\n",
    "        print(\"‚ö†Ô∏è  High reserved memory - may indicate fragmentation\")\n",
    "        print(\"   Consider kernel restart if training fails\")\n",
    "    elif reserved > total * 0.5:\n",
    "        print(\"‚ö†Ô∏è  Moderate reserved memory - should be fine for training\")\n",
    "    else:\n",
    "        print(\"‚úÖ Low reserved memory - good for training\")\n",
    "    \n",
    "    if free > 50:\n",
    "        print(\"‚úÖ Plenty of free memory available for training!\")\n",
    "    elif free > 20:\n",
    "        print(\"‚ö†Ô∏è  Limited free memory - use memory optimizations\")\n",
    "    else:\n",
    "        print(\"‚ùå Very little free memory - cleanup required!\")\n",
    "    \n",
    "    print(\"\\nüéØ RECOMMENDATION:\")\n",
    "    if free > 50 and allocated < 10:\n",
    "        print(\"‚úÖ Ready to load model and start training!\")\n",
    "    elif free > 20:\n",
    "        print(\"‚ö†Ô∏è  Can proceed, but use memory optimizations:\")\n",
    "        print(\"   - Reduce batch size\")\n",
    "        print(\"   - Enable gradient checkpointing\")\n",
    "        print(\"   - Reduce max_length\")\n",
    "    else:\n",
    "        print(\"‚ùå Need more cleanup or kernel restart\")\n",
    "        print(\"   Run clean_memory() or restart kernel\")\n",
    "\n",
    "# Run check\n",
    "check_memory_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe1d16ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f8077ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 5            |        cudaMalloc retries: 6         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    847 MiB |    847 MiB |   1006 GiB |   1005 GiB |\\n|       from large pool |    815 MiB |    815 MiB |   1005 GiB |   1004 GiB |\\n|       from small pool |     32 MiB |     32 MiB |      1 GiB |      1 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    847 MiB |    847 MiB |   1006 GiB |   1005 GiB |\\n|       from large pool |    815 MiB |    815 MiB |   1005 GiB |   1004 GiB |\\n|       from small pool |     32 MiB |     32 MiB |      1 GiB |      1 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |    844 MiB |    844 MiB |   1005 GiB |   1004 GiB |\\n|       from large pool |    812 MiB |    812 MiB |   1004 GiB |   1003 GiB |\\n|       from small pool |     32 MiB |     32 MiB |      1 GiB |      1 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  46886 MiB |  46886 MiB | 267190 MiB | 220304 MiB |\\n|       from large pool |  46824 MiB |  46824 MiB | 266716 MiB | 219892 MiB |\\n|       from small pool |     62 MiB |     62 MiB |    474 MiB |    412 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  46038 MiB |  46038 MiB | 586145 MiB | 540106 MiB |\\n|       from large pool |  46008 MiB |  46008 MiB | 584815 MiB | 538806 MiB |\\n|       from small pool |     29 MiB |     29 MiB |   1329 MiB |   1300 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     416    |     416    |   33795    |   33379    |\\n|       from large pool |      29    |      29    |    9893    |    9864    |\\n|       from small pool |     387    |     387    |   23902    |   23515    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     416    |     416    |   33795    |   33379    |\\n|       from large pool |      29    |      29    |    9893    |    9864    |\\n|       from small pool |     387    |     387    |   23902    |   23515    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      37    |      37    |     481    |     444    |\\n|       from large pool |       6    |       6    |     244    |     238    |\\n|       from small pool |      31    |      31    |     237    |     206    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      78    |      78    |   17772    |   17694    |\\n|       from large pool |      24    |      24    |    4738    |    4714    |\\n|       from small pool |      54    |      54    |   13034    |   12980    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check GPU usage\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "# 3. LOAD BASE MODEL WITH QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b2dcf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7440cd419e4db587b898a56d0abc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. LOAD BASE MODEL WITH QUANTIZATION\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Mxfp4Config\n",
    "\n",
    "quant_config = Mxfp4Config(dequantize=True)\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    dtype=torch.bfloat16,\n",
    "    quantization_config=quant_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", **model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "506cd0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,040,512 || all params: 20,929,797,696 || trainable%: 0.0719\n",
      "Number of trainable parameters with requires_grad=True: 204\n",
      "Sample trainable param requires_grad: True\n",
      "‚ö† Warning: Forward/backward test failed: name 'tokenizer' is not defined\n",
      "This might indicate an issue with quantization compatibility.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/labs/guykatz/idopinto12/projects/loop_invariant_generation/RLInv/.venv/lib/python3.13/site-packages/peft/tuners/lora/layer.py:159: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 4. WRAP WITH LORA (PEFT)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    target_parameters=[\n",
    "        \"7.mlp.experts.gate_up_proj\",\n",
    "        \"7.mlp.experts.down_proj\",\n",
    "        \"15.mlp.experts.gate_up_proj\",\n",
    "        \"15.mlp.experts.down_proj\",\n",
    "        \"23.mlp.experts.gate_up_proj\",\n",
    "        \"23.mlp.experts.down_proj\",\n",
    "    ],\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Ensure model is in training mode\n",
    "peft_model.train()\n",
    "\n",
    "# Freeze base model parameters (should already be done by PEFT, but ensure it)\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'lora' not in name.lower():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Verify trainable parameters have gradients enabled\n",
    "trainable_params = [p for p in peft_model.parameters() if p.requires_grad]\n",
    "print(f\"Number of trainable parameters with requires_grad=True: {len(trainable_params)}\")\n",
    "if len(trainable_params) > 0:\n",
    "    print(f\"Sample trainable param requires_grad: {trainable_params[0].requires_grad}\")\n",
    "\n",
    "# Test forward pass to ensure computation graph is properly connected\n",
    "# This helps identify if quantization is breaking the graph\n",
    "try:\n",
    "    test_input = tokenizer(\"Test input\", return_tensors=\"pt\").to(peft_model.device)\n",
    "    with torch.enable_grad():\n",
    "        test_output = peft_model(**test_input)\n",
    "        if hasattr(test_output, 'logits'):\n",
    "            test_loss = test_output.logits.sum()\n",
    "            # Try a backward pass to verify gradients work\n",
    "            test_loss.backward()\n",
    "            print(\"‚úì Forward and backward pass test successful - gradients are working!\")\n",
    "            # Clear gradients after test\n",
    "            peft_model.zero_grad()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Warning: Forward/backward test failed: {e}\")\n",
    "    print(\"This might indicate an issue with quantization compatibility.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9512e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. CONFIGURE TRAINING (& use wandb)\n",
    "from trl import SFTConfig\n",
    "\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_length=2048,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "    output_dir=\"gpt-oss-20b-multilingual-reasoner\",\n",
    "    report_to=\"wandb\",  # ‚Üê use wandb\n",
    "    push_to_hub=False,  # ‚Üê DO NOT push to hub\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6c446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d0379c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 10:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.811900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.829900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.614600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.572600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.156600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.230600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.121100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.943700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.964200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.977200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.920600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.936300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.992900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.936300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.094600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=1.1419532696406047, metrics={'train_runtime': 653.271, 'train_samples_per_second': 1.531, 'train_steps_per_second': 0.096, 'total_flos': 1.995467289064105e+17, 'train_loss': 1.1419532696406047, 'epoch': 1.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. START TRAINING\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07298f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. SAVE MODEL LOCALLY (no pushing to hub)\n",
    "trainer.save_model(training_args.output_dir)\n",
    "\n",
    "del trainer\n",
    "del peft_model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d473719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['reasoning_language', 'developer', 'user', 'analysis', 'final', 'messages'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6aabd1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'developer',\n",
       "   'content': 'You are a helpful assistant and an expert C programmer.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Generate a strong loop invariant that helps prove the target property of the following C program: \\n```c\\n{program}\\n```\\n\\nAvailable locations for placing the invariant:\\n{locations}\\n\\nOutput Format:\\nassert(<invariant>); // Line <line_number>'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'assert(<invariant>); // Line <line_number>'}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dataset/training/train_format.jsonl\", \"r\") as f:\n",
    "    train_format = json.load(f)\n",
    "    \n",
    "train_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "17043c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-09\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a helpful assistant and an expert C programmer.\n",
      "\n",
      "<|end|><|start|>user<|message|>Generate a strong loop invariant that helps prove the target property of the following C program: \n",
      "```c\n",
      "{program}\n",
      "```\n",
      "\n",
      "Available locations for placing the invariant:\n",
      "{locations}\n",
      "\n",
      "Output Format:\n",
      "assert(<invariant>); // Line <line_number><|end|><|start|>assistant<|channel|>final<|message|>assert(<invariant>); // Line <line_number><|return|>\n"
     ]
    }
   ],
   "source": [
    "conversation = tokenizer.apply_chat_template(train_format['messages'], tokenize=False)\n",
    "print(conversation)  # Shows how dialogue is formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ad7e77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "726"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d49f40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-11-09\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a helpful assistant and an expert C programmer.\\n\\n<|end|><|start|>user<|message|>Generate a strong loop invariant that helps prove the target property of the following C program: \\n```c\\n{program}\\n```\\n\\nAvailable locations for placing the invariant:\\n{locations}\\n\\nOutput Format:\\nassert(<invariant>); // Line <line_number><|end|><|start|>assistant<|channel|>final<|message|>assert(<invariant>); // Line <line_number><|return|>'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a293640b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to create a HuggingFace Dataset from train_format (dict loaded from JSON)?\n",
    "from datasets import Dataset\n",
    "\n",
    "# Here train_format[\"messages\"] is a list of turn dicts, but we want to keep the list-of-dicts per example.\n",
    "# For 1 conversation example:\n",
    "\n",
    "# If you have a list of conversations (list-of-dicts, each with \"messages\" field), you can do:\n",
    "# dataset = Dataset.from_list(list_of_conversations)\n",
    "conversations = [{'messages': train_format['messages']}]\n",
    "my_dataset = Dataset.from_list(conversations)\n",
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "125462b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a helpful assistant and an expert C programmer.',\n",
       "   'role': 'developer'},\n",
       "  {'content': 'Generate a strong loop invariant that helps prove the target property of the following C program: \\n```c\\n{program}\\n```\\n\\nAvailable locations for placing the invariant:\\n{locations}\\n\\nOutput Format:\\nassert(<invariant>); // Line <line_number>',\n",
       "   'role': 'user'},\n",
       "  {'content': 'assert(<invariant>); // Line <line_number>',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a736f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Paths\n",
    "training_folder = \"dataset/training\"\n",
    "programs_path = os.path.join(training_folder, \"Programs\")\n",
    "\n",
    "invariants_path = os.path.join(training_folder, \"invariants.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07317cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load invariants.json\n",
    "with open(invariants_path, 'r') as f:\n",
    "    invariants_data = json.load(f)\n",
    "\n",
    "# Get first 5 file names\n",
    "files = list(invariants_data.keys())[:5]\n",
    "\n",
    "# Build list of dicts for each file: {'code', 'invariant', 'line'}\n",
    "data  = []\n",
    "for filename in files:\n",
    "    # Try to read code\n",
    "    program_path = os.path.join(programs_path, filename)\n",
    "    try:\n",
    "        with open(program_path, 'r') as code_file:\n",
    "            code = code_file.read()\n",
    "    except FileNotFoundError:\n",
    "        code = None  # or handle otherwise\n",
    "\n",
    "    # Get invariants for this file (may be a list)\n",
    "    invariants = invariants_data[filename]\n",
    "    # For each invariant/line pair, make a dict, but include the code each time\n",
    "    for inv in invariants:\n",
    "        data.append({\n",
    "            'code': code,\n",
    "            'invariant': inv.get('invariant'),\n",
    "            'line': inv.get('line'),\n",
    "            'file': filename\n",
    "        })\n",
    "\n",
    "# Now, first_5_data is a list of dicts, one per (filename, invariant/line).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f8a4e235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'code': '#include <assert.h>\\nvoid reach_error(void) { assert(0); }\\n\\nextern int __VERIFIER_nondet_int(void);\\nextern _Bool __VERIFIER_nondet_bool(void);\\n\\nvoid __VERIFIER_assert(int cond) {\\n    if (!cond) {\\n        reach_error();\\n    }\\n}\\n\\n/* Custom CFG:\\nnames=i count isPositive\\nbeforeloop=\\nbeforeloopinit=\\nprecondition=i==0 && count==0 && isPositive==1\\nloopcondition=i<200 && isPositive\\nloop=count += i; i += 2; isPositive = (i % 5 != 0);\\npostcondition=count >= 0\\nafterloop=\\nlearners= conj\\n*/\\nint main() {\\n    int i = __VERIFIER_nondet_int();\\n    int count = __VERIFIER_nondet_int();\\n    _Bool isPositive = __VERIFIER_nondet_bool();\\n\\n    if (!(i == 0 && count == 0 && isPositive == 1)) {\\n        return 0;\\n    }\\n\\n    while (i < 200 && isPositive) {\\n        count += i;\\n        i += 2;\\n        isPositive = (i % 5 != 0);\\n    }\\n\\n    __VERIFIER_assert(count >= 0);\\n    return 0;\\n}',\n",
       "  'invariant': '0 <= count && 0 <= i',\n",
       "  'line': 33,\n",
       "  'file': '1003_1.c'},\n",
       " {'code': '#include <assert.h>\\n#include <stdlib.h>\\n\\n// Function prototypes for nondeterministic inputs and error handling\\nextern int __VERIFIER_nondet_int();\\nextern void abort(void);\\nvoid reach_error() { assert(0); }\\n\\n// Function to assert a condition\\nvoid __VERIFIER_assert(int cond) {\\n    if (!cond) {\\n    ERROR : {\\n        reach_error();\\n        abort();\\n    }\\n    }\\n    return;\\n}\\n\\n// Function to calculate factorial iteratively\\nint factorial(int n) {\\n    int result = 1;\\n    for (int i = 1; i <= n; i++) {\\n        result *= i;\\n    }\\n    return result;\\n}\\n\\nint main(void) {\\n    // Get a nondeterministic integer input\\n    int n = __VERIFIER_nondet_int();\\n\\n    // Constrain n to be within a valid range\\n    if (n >= 0 && n <= 10) {\\n        int fact = factorial(n);\\n\\n        // Verify factorial property: n! > 0 for n >= 0\\n        __VERIFIER_assert(fact > 0);\\n\\n        // Additional property: 0! == 1\\n        if (n == 0) {\\n        }\\n\\n        // Basic loop to check properties, demonstrating loop invariants\\n        int sum = 0;\\n        for (int i = 0; i <= n; i++) {\\n            sum += i;\\n        }\\n\\n        // Verify sum of first n natural numbers: sum == n * (n + 1) / 2\\n    }\\n\\n    return 0;\\n}',\n",
       "  'invariant': '1 <= result && 1 <= i',\n",
       "  'line': 23,\n",
       "  'file': '1009_1.c'},\n",
       " {'code': '#include <assert.h>\\n#include <stdlib.h>\\n\\n// Function prototypes for nondeterministic inputs and error handling\\nextern int __VERIFIER_nondet_int();\\nextern void abort(void);\\nvoid reach_error() { assert(0); }\\n\\n// Function to assert a condition\\nvoid __VERIFIER_assert(int cond) {\\n    if (!cond) {\\n    ERROR : {\\n        reach_error();\\n        abort();\\n    }\\n    }\\n    return;\\n}\\n\\n// Function to calculate factorial iteratively\\nint factorial(int n) {\\n    int result = 1;\\n    for (int i = 1; i <= n; i++) {\\n        result *= i;\\n    }\\n    return result;\\n}\\n\\nint main(void) {\\n    // Get a nondeterministic integer input\\n    int n = __VERIFIER_nondet_int();\\n\\n    // Constrain n to be within a valid range\\n    if (n >= 0 && n <= 10) {\\n        int fact = factorial(n);\\n\\n        // Verify factorial property: n! > 0 for n >= 0\\n\\n        // Additional property: 0! == 1\\n        if (n == 0) {\\n            __VERIFIER_assert(fact == 1);\\n        }\\n\\n        // Basic loop to check properties, demonstrating loop invariants\\n        int sum = 0;\\n        for (int i = 0; i <= n; i++) {\\n            sum += i;\\n        }\\n\\n        // Verify sum of first n natural numbers: sum == n * (n + 1) / 2\\n    }\\n\\n    return 0;\\n}',\n",
       "  'invariant': '1 <= n || ((1 <= result && result <= 1) && !(i <= n))',\n",
       "  'line': 23,\n",
       "  'file': '1009_2.c'},\n",
       " {'code': '#include <assert.h>\\n#include <stdlib.h>\\nvoid reach_error() { assert(0); }\\n\\nvoid __VERIFIER_assert(int cond) {\\n    if (!(cond)) {\\n    ERROR : {\\n        reach_error();\\n        abort();\\n    }\\n    }\\n    return;\\n}\\n\\nextern int __VERIFIER_nondet_int(void);\\nextern void abort(void);\\n\\nvoid assume_abort_if_not(int cond) {\\n    if (!cond) {\\n        abort();\\n    }\\n}\\n\\nint main() {\\n    int n = __VERIFIER_nondet_int();\\n    int sum = 0, prod = 1;\\n\\n    // Assuming n is between 1 and 100\\n    assume_abort_if_not(n > 0 && n <= 100);\\n\\n    // Calculate sum of first n integers and factorial of n\\n    while (n > 0) {\\n        sum = sum + n;\\n        prod = prod * n;\\n        n = n - 1;\\n\\n        __VERIFIER_assert(sum >= 0);  // sum should always be non-negative\\n        __VERIFIER_assert(prod >= 1); // prod should always be positive\\n    }\\n\\n    // To verify the mathematical correctness of sum\\n    // sum should be n*(n+1)/2 if loop runs correctly from original n to 0\\n    __VERIFIER_assert(sum == n * (n + 1) / 2 || (n == 0));\\n\\n    return 0;\\n}',\n",
       "  'invariant': '((1 <= sum && 1 <= prod) && 0 <= n) || (((((sum >= 0) == 1 && 0 < n) && n <= 100) && 0 <= sum) && prod == 1)',\n",
       "  'line': 32,\n",
       "  'file': '100_1.c'},\n",
       " {'code': '#include <assert.h>\\nvoid reach_error(void) { assert(0); }\\n\\nextern int __VERIFIER_nondet_int(void);\\nextern _Bool __VERIFIER_nondet_bool(void);\\n\\nvoid __VERIFIER_assert(int cond) {\\n    if (!cond) {\\n        reach_error();\\n    }\\n}\\n\\n/*\\nExample program with constraints:\\nnames=a b\\nprecondition=a > 0 && b > a\\nloopcondition=b >= a\\nloop=a = a * 2; b = b - 3;\\npostcondition=a >= b && b < a * 2 - 1\\nlearners=lin\\n*/\\nint main() {\\n    int a = __VERIFIER_nondet_int();\\n    int b = __VERIFIER_nondet_int();\\n\\n    // Precondition checks\\n    if (!(a > 0 && b > a)) {\\n        return 0;\\n    }\\n\\n    // Main loop with constraints\\n    while (b >= a) {\\n        a = a * 2;\\n        b = b - 3;\\n    }\\n\\n    // Postcondition to satisfy\\n    __VERIFIER_assert(a >= b && b < a * 2 - 1);\\n\\n    return 0;\\n}',\n",
       "  'invariant': '(1 <= a && a + 1 <= b) || 2 <= a',\n",
       "  'line': 32,\n",
       "  'file': '101_1.c'}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7ec15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': 'You are a helpful assistant and an expert C programmer.'}, {'role': 'user', 'content': 'Generate a strong loop invariant that helps prove the target property of the following C program: \\n```c\\n#include <assert.h>\\nvoid reach_error(void) { assert(0); }\\n\\nextern int __VERIFIER_nondet_int(void);\\nextern _Bool __VERIFIER_nondet_bool(void);\\n\\nvoid __VERIFIER_assert(int cond) {\\n    if (!cond) {\\n        reach_error();\\n    }\\n}\\n\\n/* Custom CFG:\\nnames=i count isPositive\\nbeforeloop=\\nbeforeloopinit=\\nprecondition=i==0 && count==0 && isPositive==1\\nloopcondition=i<200 && isPositive\\nloop=count += i; i += 2; isPositive = (i % 5 != 0);\\npostcondition=count >= 0\\nafterloop=\\nlearners= conj\\n*/\\nint main() {\\n    int i = __VERIFIER_nondet_int();\\n    int count = __VERIFIER_nondet_int();\\n    _Bool isPositive = __VERIFIER_nondet_bool();\\n\\n    if (!(i == 0 && count == 0 && isPositive == 1)) {\\n        return 0;\\n    }\\n\\n    while (i < 200 && isPositive) {\\n        count += i;\\n        i += 2;\\n        isPositive = (i % 5 != 0);\\n    }\\n\\n    __VERIFIER_assert(count >= 0);\\n    return 0;\\n}\\n```\\n\\nAvailable locations for placing the invariant:\\n33\\n\\nOutput Format:\\nassert(<invariant>); // Line <line_number>'}, {'role': 'assistant', 'content': 'assert(0 <= count && 0 <= i); // Line 33'}]}\n"
     ]
    }
   ],
   "source": [
    "developer_msg = \"You are a helpful assistant and an expert C programmer.\"\n",
    "user_msg = \"Generate a strong loop invariant that helps prove the target property of the following C program: \\n```c\\n{program}\\n```\\n\\nAvailable locations for placing the invariant:\\n{locations}\\n\\nOutput Format:\\nassert(<invariant>); // Line <line_number>\"\n",
    "response_msg = \"assert({invariant}); // Line {line_number}\"\n",
    "samples = []\n",
    "for example in data:\n",
    "    sample = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": developer_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg.format(program=example['code'], locations=example['line'])},\n",
    "            {\"role\": \"assistant\", \"content\": response_msg.format(invariant=example['invariant'], line_number=example['line'])}\n",
    "        ]\n",
    "    }\n",
    "    samples.append(sample)\n",
    "print(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9425d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
