wandb_project: "rlinv-sft"
model_name: "openai/gpt-oss-20b"
hf_dataset: "idopinto/gen-inv-sft-for-gpt-oss-full-sep2"

# datasets
# idopinto/gen-inv-sft-for-gpt-oss-full-sep2 (3,903)
# idopinto/gen-inv-sft-for-gpt-oss-full2 (3,125)
limit: -1 # 3,903

sft_config:
  # Option 1: Evaluate every 4 steps (will trigger at least once)
  eval_strategy: "steps"
  eval_steps: 30
  
  # Option 2: Evaluate at end of each epoch (simpler)
  # eval_strategy: "epoch"
  # (remove eval_steps or set to null)
  # assistant_only_loss: True
  learning_rate: 0.0002
  gradient_checkpointing: True
  num_train_epochs: 3
  # max_steps: 30
  logging_steps: 1
  per_device_train_batch_size: 12
  gradient_accumulation_steps: 6
  max_length: 1024
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs: {"min_lr_rate": 0.1}
  output_dir: "gpt-oss-20b-rlinv-sft-sep"
  report_to: "wandb"
  push_to_hub: True

lora_config:
  r: 8
  lora_alpha: 16
  target_modules: "all-linear"
  target_parameters:
    - "7.mlp.experts.gate_up_proj"
    - "7.mlp.experts.down_proj"
    - "15.mlp.experts.gate_up_proj"
    - "15.mlp.experts.down_proj"
    - "23.mlp.experts.gate_up_proj"
    - "23.mlp.experts.down_proj"
  lora_dropout: 0.0
  bias: "none"
  task_type: "CAUSAL_LM"


model_train_config:
  attn_implementation: "eager"
  dtype: "bfloat16"
  # quantization_config: "Mxfp4Config(dequantize=True)"
  use_cache: False
  device_map: "auto"
  # reasoning_effort: "medium"

model_eval_config:
  max_new_tokens: 4096
  # reasoning_effort: "medium"
  do_sample: True
  temperature: 0.6
  top_p: 0.9